#!/usr/bin/python

# s3wipe - Eric Schwimmer (eric@nerdvana.org)
#
# A python script to rapidly delete all of the objects contained within
# an S3 bucket, as well as the bucket  itself.  Running from an EC2
# instance in the same region as the bucket it is deleting, this script
# can delete around 5000 objects/second (including versioned objects).
#
# Licensed via the MIT License (http://opensource.org/licenses/MIT)

import argparse, Queue, logging, random, sys
import multiprocessing, signal
from multiprocessing.pool import ThreadPool

version = "0.1"

# Make sure we have a semi-recent version of boto installed
try:
    import boto.s3.connection                                  
    if tuple(map(int, boto.__version__.split("."))) < (2,5,2): 
        raise Exception
except:
    print "ERROR: s3wipe requires boto v2.5.2 or later!"
    sys.exit(1)

# Fetch our command line arguments
def getArgs():
    parser = argparse.ArgumentParser(
        prog="s3wipe",
        description="Delete all keys in an S3 bucket",
        formatter_class=lambda prog: argparse.HelpFormatter(prog,max_help_position=27))

    parser.add_argument("--bucket", 
        help="Bucket name to delete", required=True)
    parser.add_argument("--id", 
        help="Your AWS access key ID", required=True)
    parser.add_argument("--key", 
        help="Your AWS secret access key", required=True)
    parser.add_argument("--quiet", 
        help="Suprress all non-error output", action='store_true')
    parser.add_argument("--batchSize", 
        help="# of keys to batch delete (default 100)", 
        type=int, default=100)
    parser.add_argument("--maxQueue", 
        help="Max size of deletion queue (default 10k)", 
        type=int, default=10000)

    return parser.parse_args()

# Set up our logging object
def loggerSetup(args):
                                                                              
    # Set our maximum severity level to log (i.e. debug or not)               
    if args.quiet:
        logLevel = logging.ERROR
    else:
        logLevel = logging.DEBUG
                                                                              
    # Log configuration                                                       
    logging.basicConfig(                                              
        level=logLevel,                                               
        format="%(asctime)s %(levelname)s: %(message)s",              
        datefmt="[%Y-%m-%d@%H:%M:%S]"                                 
    )                                                                 
                                                                              
    # Create logger and point it at our log file                              
    global logger                                                             
    logger = logging.getLogger("s3wipe")                                     
                                                                              
    # Make the logger emit all unhandled exceptions                           
    sys.excepthook = lambda t, v, x: logger.error(
        "Uncaught exception", exc_info=(t,v,x))
                                                             
    # Supress boto debug logging, since it is very chatty    
    logging.getLogger("boto").setLevel(logging.CRITICAL)     


# Our deletion worker, called by Threadpool
def deleter(args, rmQueue, numThreads):

    # Set up per-thread boto objects
    myconn = boto.s3.connection.S3Connection(                                  
        aws_access_key_id=args.id,
        aws_secret_access_key=args.key)
    mybucket = myconn.get_bucket(args.bucket)

    done = False
    rmKeys = []

    while True:
        # Snatch a key off our deletion queue and add it
        # to our local deletion list
        rmKey = rmQueue.get()
        rmKeys.append(rmKey)

        # Poll our deletion queue until it is empty or
        # until we have accumulated enough keys in this
        # thread's delete list to justify a batch delete
        if len(rmKeys) >= args.batchSize or rmQueue.empty():
            try:
                mybucket.delete_keys(rmKeys)
            except:
                continue

            with keysDeleted.get_lock():
                keysDeleted.value += len(rmKeys)
            rmKeys = []

            # Print some progress info
            if random.randint(0,numThreads) == numThreads:
                logger.info("Deleted %s out of %s keys found thus far.",
                    keysDeleted.value, keysFound.value)

        rmQueue.task_done()

# Set the global vars for our listing threads
def listInit(arg1, arg2):
    global args, rmQueue
    args = arg1
    rmQueue = arg2

# Our listing worker, which will poll the s3 bucket mericlessly and
# insert all objects found into the deletion queue.
def lister(subDir):

    # Set up our per-thread boto connection
    myconn = boto.s3.connection.S3Connection(                                  
        aws_access_key_id=args.id,
        aws_secret_access_key=args.key)
    mybucket = myconn.get_bucket(args.bucket)

    # Iterate through bucket and enqueue all keys found in
    # our deletion queue
    for key in mybucket.list_versions(prefix=subDir.name):
        rmQueue.put(key)
        with keysFound.get_lock():
            keysFound.value += 1

# Our main function
def main():

    # Parse arguments
    args = getArgs()

    # Set up the logging object
    loggerSetup(args)

    rmQueue = Queue.Queue(maxsize=args.maxQueue)

    # Catch ctrl-c to exit cleanly
    signal.signal(signal.SIGINT, lambda x,y: sys.exit(0))

    # Our thread-safe variables, used for progress tracking
    global keysFound, keysDeleted
    keysFound = multiprocessing.Value("i",0)
    keysDeleted = multiprocessing.Value("i",0)

    logger.info("Getting bucket subdirs to feed to list threads")

    # Our main boto object.  Really only used to start the
    # watcher threads on a per-subdir basis
    conn = boto.s3.connection.S3Connection(                                  
        aws_access_key_id=args.id,
        aws_secret_access_key=args.key)

    try:
        bucket = conn.get_bucket(args.bucket)

    except boto.s3.connection.S3ResponseError as e:
        shortErr = str(e).split('\n')[0]                
        logger.error(shortErr)
        sys.exit(1)

    bucket.configure_versioning(True)

    # Poll the root-level directories in the s3 bucket, and
    # start a reader process for each one of them
    subDirs = list(bucket.list_versions(delimiter="/"))
    listThreads = len(subDirs)
    deleteThreads = listThreads*2

    # Now start all of our delete & list threads
    if listThreads > 0:
        logger.info("Starting %s delete threads..." % deleteThreads)
        deleterPool = ThreadPool(processes=deleteThreads, 
            initializer=deleter, initargs=(args, rmQueue, deleteThreads))

        logger.info("Starting %s list threads..." % listThreads)
        listerPool = ThreadPool(processes=listThreads, 
            initializer=listInit, initargs=(args, rmQueue))

        # Feed the root-level subdirs to our listing process, which
        # will in-turn populate the deletion queue, which feed the
        # deletion threads
        listerPool.map(lister, subDirs)
        rmQueue.join()

    logger.info("Bucket is empty.  Attempting to remove bucket")
    conn.delete_bucket(args.bucket)


if __name__ == "__main__":
    main()
